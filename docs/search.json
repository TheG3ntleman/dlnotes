[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning Book",
    "section": "",
    "text": "A collection of notes related to deep learning."
  },
  {
    "objectID": "pytorch-geometric.html",
    "href": "pytorch-geometric.html",
    "title": "2  Pytorch Geometric",
    "section": "",
    "text": "3 Applying classical deep learning techniques to graph based data"
  },
  {
    "objectID": "pytorch-geometric.html#every-graph-can-be-represented-as-an-adjacency-matrix-and-there-already-exist-a-wide-variety-of-architectures-in-deep-learning-which-act-primarily-on-matrices-primarily-those-related-to-image-processing.-however-passing-the-adjacency-matrix-of-a-graph-as-input-to-a-deep-neural-network-is-a-bad-idea-for-a-variety-of-reasons",
    "href": "pytorch-geometric.html#every-graph-can-be-represented-as-an-adjacency-matrix-and-there-already-exist-a-wide-variety-of-architectures-in-deep-learning-which-act-primarily-on-matrices-primarily-those-related-to-image-processing.-however-passing-the-adjacency-matrix-of-a-graph-as-input-to-a-deep-neural-network-is-a-bad-idea-for-a-variety-of-reasons",
    "title": "2  Pytorch Geometric",
    "section": "3.1 Every graph can be represented as an adjacency matrix and there already exist a wide variety of architectures in deep learning which act primarily on matrices primarily those related to image processing. However, passing the adjacency matrix of a graph as input to a deep neural network is a bad idea for a variety of reasons:",
    "text": "3.1 Every graph can be represented as an adjacency matrix and there already exist a wide variety of architectures in deep learning which act primarily on matrices primarily those related to image processing. However, passing the adjacency matrix of a graph as input to a deep neural network is a bad idea for a variety of reasons:"
  },
  {
    "objectID": "tutorial-1.html",
    "href": "tutorial-1.html",
    "title": "3  What is pytorch geometric?",
    "section": "",
    "text": "4 Graph SAGE\nGraph SAGE is a architecture that is mostly the same as the general GNN described above with the exception of the propagation and aggregation mechanism, which we describe here, \\[h_v^{k+1} = \\sigma\\bigg(\\bigg[W_k\\cdot AGG\\bigg(\\{h_u^{k-1}, \\forall u\\in N(v)\\}\\bigg), B_kh_v^k\\bigg]\\bigg)\\] where, \\([,]\\) is a concatenation and \\(AGG\\) is a general aggregation strategy.\n# Tutorial - I code from this point on\nimport torch_geometric\nfrom torch_geometric.datasets import Planetoid\ndataset = Planetoid(root=\"tutorial1\", name=\"Cora\")\nprint(dataset)\nprint(\"number of graphs:\\t\\t\", len(dataset))\nprint(\"number of classes:\\t\\t\", dataset.num_classes)\nprint(\"number of node features:\\t\", dataset.num_node_features)\nprint(\"number of edge features:\\t\", dataset.num_edge_features)\n\nCora()\nnumber of graphs:        1\nnumber of classes:       7\nnumber of node features:     1433\nnumber of edge features:     0\nimport os.path as osp\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\ndata = dataset[0]\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv = SAGEConv(dataset.num_features,\n                            dataset.num_classes,\n                            aggr = \"max\")\n        \n    def forward(self):\n        x = self.conv(data.x, data.edge_index)\n        return F.log_softmax(x, dim=1)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel, data = Net().to(device), data.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n    optimizer.step()\n\ndef test():\n    model.eval()\n    logits, accs = model(), []\n    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n        pred = logits[mask].max(1)[1]\n        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n        accs.append(acc)\n    return accs\nbest_val_acc = test_acc = 0\nfor epoch in range(1, 100):\n    train()\n    _, val_acc, tmp_test_acc = test()\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        test_acc = tmp_test_acc\n    log = 'Epoch: {:03d}, Val: {:.4f}, Test: {:.4f}'\n    \n    if epoch % 10 == 0:\n        print(log.format(epoch, best_val_acc, test_acc))\n\nEpoch: 010, Val: 0.7120, Test: 0.7340\nEpoch: 020, Val: 0.7120, Test: 0.7340\nEpoch: 030, Val: 0.7120, Test: 0.7340\nEpoch: 040, Val: 0.7120, Test: 0.7340\nEpoch: 050, Val: 0.7120, Test: 0.7340\nEpoch: 060, Val: 0.7120, Test: 0.7340\nEpoch: 070, Val: 0.7120, Test: 0.7340\nEpoch: 080, Val: 0.7120, Test: 0.7340\nEpoch: 090, Val: 0.7120, Test: 0.7340"
  },
  {
    "objectID": "tutorial-1.html#classical-deep-learning-on-graph-based-data",
    "href": "tutorial-1.html#classical-deep-learning-on-graph-based-data",
    "title": "3  What is pytorch geometric?",
    "section": "3.1 Classical deep learning on graph-based data",
    "text": "3.1 Classical deep learning on graph-based data\nThe naive approach to this problem is to find the adjacency matrix of a input graph and use that as input to a neural network as there are already several successful architectures that take matrices as input, primarily those related to computer vision (ex. convolutional neural networks, feed forward neural networks, …). However, this approach has some problems: - Model cannot adapt to changes in graph size (I.E number of nodes). - The adjacency matrix is not permutation invariant (I.E one graph might have multiple distinct adjacency matrices). In many cases, not only is the structure of the graph important but also the properties of the nodes in a particular graph. Thus, in a general case, a network must also account for a secondary features matrix containing node related information."
  },
  {
    "objectID": "tutorial-1.html#graph-neural-networks",
    "href": "tutorial-1.html#graph-neural-networks",
    "title": "3  What is pytorch geometric?",
    "section": "3.2 Graph Neural Networks",
    "text": "3.2 Graph Neural Networks\n\n3.2.1 The computation graph of a node\nEvery node of a graph has a computation graph. The computation graph of a node is a tree where the root node is the node in question and it’s children are it’s neighbours. The neighbours in the child layer also have children (composed of the neighbours’s neighbours). This process continuous until all nodes are exhuasted. Node repetition is valid.\nSometimes, the process of generating the this computation graph layer by layer is informally referred to as unrolling the graph about a particular node. Often unrolling is limited to a particular number, this imposes a constraint on the depth of each computation graph.\n\n\n3.2.2 Action on the computation graph of a node\nGNNs process graph-data using two mechanisms: aggregation and propagation. Starting from the leaf-nodes, data is supplied from the node-feature matrix. If multiple children share a parent, their information is combined through aggregation. The aggregated product is then transmitted to the parent, considering the parent’s feature vector, via the propagation mechanism. This process continues until information reaches the root node.\nAn important property of the aggregation mechanism is its permutation invariance, meaning it doesn’t depend on the order of inputs supplied.\n\n\n3.2.3 How much to unroll\nUnrolling too little can result in important information not being transmitted whereas unrolling too much makes computation far more expensive even though the information transmitted from distant nodes might not affect the overall result.\n\n\n3.2.4 Computation formally described for a particular case\nThe above computation can be consicely described by the following equations: \\[\n\\begin{gather}\nH_v^0 = X_v \\\\\nh_v^{k+1} = \\sigma\\bigg(W_k\\sum_{u\\in N(v)}\\frac{h_u^k}{|N(v)|} + B_kh_v^k\\bigg) \\\\\nZ_v = h_v^K\n\\end{gather}\n\\] where, \\(\\sigma\\) is an activation function \\(W_k\\) and \\(B_k\\) are trainable weights, \\(N(u)\\) is the set of neighbours of node \\(u\\), and, \\(\\sum\\) is some permutation invariant aggregation function. Keep in mind that this equation only comes after making a specific selection as to how the propagation mechanism acts."
  },
  {
    "objectID": "tutorial-2.html",
    "href": "tutorial-2.html",
    "title": "4  stuff covered",
    "section": "",
    "text": "Datasets\nModels\nLosses\nOptimizers\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n\n5 Datasets\n\n5.0.1 A raw dataset\nThe data are obtained from the model\n\ninput_dim = 1\noutput_dim = 1\n\nA = 2 * np.random.rand(output_dim, input_dim) - 1\nb = 2 * np.random.rand(output_dim) - 1\n\ntrue_model = lambda x : A @ x + b\n\n\nn_train = 1000\nnoise_level  = 0.04\n\nX_train = np.random.rand(n_train, input_dim)\ny_train = np.array([true_model(x) for x in X_train])\n\ny_train += noise_level * np.random.standard_normal(size=y_train.shape)\n\n\nif input_dim == output_dim == 1:\n    fig = plt.figure()\n    fig.clf()\n    ax = fig.clf()\n    ax =fig.gca()\n    ax.plot(X_train, y_train, '.')\n    ax.grid(True)\n    ax.set_xlabel('X_train')\n    ax.set_ylabel('y_train')\n\n\n\n\n\n\n\n6 Pytorch dataset\n\nimport torch\n\nclass VectorialDataset(torch.utils.data.Dataset):\n    def __init__(self, input_data, output_data):\n        super(VectorialDataset, self).__init__()\n        self.input_data = torch.tensor(input_data.astype('f'))\n        self.output_data = torch.tensor(output_data.astype('f'))\n    \n    def __len__(self):\n        return self.input_data.shape[0]\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        sample = (self.input_data[idx, :], self.output_data[idx, :])\n        return sample\n\n\ntraining_set = VectorialDataset(input_data = X_train,\n                               output_data = y_train)\n\nlen(training_set)\n\n1000\n\n\n\ntraining_set[10:12]\n\n(tensor([[0.8926],\n         [0.8290]]),\n tensor([[-0.2250],\n         [-0.2084]]))\n\n\n\nbatch_size = 120\ntrain_loader = torch.utils.data.DataLoader(training_set,\n                                         batch_size = batch_size,\n                                         shuffle=True)\n\n\nfor idx, batch in enumerate(train_loader):\n    print(\"Batch n. %2d: input size = %s, output size = %s\" % (idx + 1, batch[0].shape, batch[1].shape))\n\nBatch n.  1: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  2: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  3: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  4: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  5: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  6: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  7: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  8: input size = torch.Size([120, 1]), output size = torch.Size([120, 1])\nBatch n.  9: input size = torch.Size([40, 1]), output size = torch.Size([40, 1])\n\n\n\nfirst_batch = []\n\nfor epoch in range(2):\n    for idx, batch in enumerate(train_loader):\n        if idx == 0:\n            first_batch.append(batch)\n\nnp.c_[X_train[:batch_size], first_batch[0][0].numpy(), first_batch[1][0].numpy()]\n\narray([[0.99441294, 0.88566828, 0.90638602],\n       [0.71332327, 0.25848401, 0.09608797],\n       [0.3851813 , 0.64516836, 0.68892878],\n       [0.31010884, 0.85967344, 0.37709296],\n       [0.05905315, 0.281896  , 0.47578371],\n       [0.93848683, 0.29293627, 0.00753411],\n       [0.50167444, 0.54246819, 0.48315424],\n       [0.79054474, 0.89020598, 0.77863574],\n       [0.99751762, 0.22161275, 0.10783614],\n       [0.96920575, 0.72366333, 0.03943504],\n       [0.89263485, 0.45239502, 0.03817135],\n       [0.82903649, 0.42025983, 0.80356568],\n       [0.66041292, 0.82903647, 0.59217465],\n       [0.54318451, 0.57972568, 0.30210105],\n       [0.12219063, 0.7035467 , 0.01280502],\n       [0.12336011, 0.50322264, 0.78283191],\n       [0.57840621, 0.39421234, 0.26313797],\n       [0.60122219, 0.28883293, 0.0648143 ],\n       [0.23635884, 0.43369856, 0.96331966],\n       [0.49244715, 0.87910348, 0.23238443],\n       [0.76760602, 0.74016517, 0.81064886],\n       [0.124236  , 0.34805065, 0.7856667 ],\n       [0.99684241, 0.58950341, 0.58950341],\n       [0.17274168, 0.77288872, 0.35998109],\n       [0.93988512, 0.09608797, 0.24329041],\n       [0.88006585, 0.45752376, 0.01388622],\n       [0.30657252, 0.58852458, 0.69125623],\n       [0.69952798, 0.79163045, 0.48857278],\n       [0.88943682, 0.24400333, 0.9872328 ],\n       [0.55080383, 0.92173958, 0.32762069],\n       [0.51320146, 0.21320648, 0.68602443],\n       [0.96090869, 0.828637  , 0.93523026],\n       [0.72400397, 0.03687207, 0.56985676],\n       [0.02360363, 0.3668623 , 0.2627891 ],\n       [0.52421809, 0.38933042, 0.81937206],\n       [0.95005384, 0.06986201, 0.11641955],\n       [0.09956453, 0.79054475, 0.57839799],\n       [0.44530446, 0.06964737, 0.23636465],\n       [0.71552746, 0.48218346, 0.68855178],\n       [0.41688852, 0.96209741, 0.828637  ],\n       [0.89920069, 0.93988514, 0.13591732],\n       [0.41888738, 0.25178304, 0.85155219],\n       [0.01280502, 0.95430303, 0.99684238],\n       [0.58086366, 0.67945826, 0.34696212],\n       [0.88979192, 0.94928753, 0.18972977],\n       [0.0982474 , 0.38136426, 0.09368064],\n       [0.9942374 , 0.0221359 , 0.58914739],\n       [0.31588371, 0.96090871, 0.87985909],\n       [0.11780005, 0.98523474, 0.8765052 ],\n       [0.04699188, 0.96071446, 0.68318546],\n       [0.78509243, 0.65596133, 0.68043625],\n       [0.34406435, 0.22218627, 0.12874915],\n       [0.17844747, 0.86949039, 0.24557385],\n       [0.61174964, 0.22043672, 0.14361537],\n       [0.32081802, 0.12752514, 0.62298632],\n       [0.16325195, 0.6324504 , 0.51320148],\n       [0.59267516, 0.2627891 , 0.17386498],\n       [0.89435327, 0.27717113, 0.49589038],\n       [0.99559236, 0.72400397, 0.01518937],\n       [0.84451628, 0.81937206, 0.71446478],\n       [0.37893227, 0.30432567, 0.04866788],\n       [0.61828079, 0.27384785, 0.89980233],\n       [0.52317273, 0.96006864, 0.86317033],\n       [0.29240602, 0.11641955, 0.16851683],\n       [0.78179026, 0.61130887, 0.52317274],\n       [0.87650519, 0.14638011, 0.55160248],\n       [0.76234368, 0.63475406, 0.05731268],\n       [0.59705291, 0.92635489, 0.55611342],\n       [0.05280673, 0.33815414, 0.38518131],\n       [0.03624628, 0.13451321, 0.71625119],\n       [0.67292853, 0.02615269, 0.88402104],\n       [0.49495131, 0.46950278, 0.94179547],\n       [0.24508904, 0.03522599, 0.51866555],\n       [0.0767747 , 0.88006586, 0.4929468 ],\n       [0.8526227 , 0.77501941, 0.34475401],\n       [0.98155471, 0.4186168 , 0.28138912],\n       [0.34949342, 0.58146203, 0.57018268],\n       [0.6416756 , 0.80419493, 0.593328  ],\n       [0.1233906 , 0.62320602, 0.90519118],\n       [0.22137517, 0.18972977, 0.25887921],\n       [0.0648143 , 0.79898971, 0.79769021],\n       [0.42189795, 0.73690206, 0.3158837 ],\n       [0.5507275 , 0.69952798, 0.64492881],\n       [0.98661093, 0.95618361, 0.16329598],\n       [0.06230946, 0.66737002, 0.63083929],\n       [0.96090702, 0.33282843, 0.7380231 ],\n       [0.13285339, 0.9750644 , 0.56930441],\n       [0.34500432, 0.90361249, 0.0319028 ],\n       [0.62125416, 0.49505863, 0.88696581],\n       [0.78283188, 0.57830667, 0.42025983],\n       [0.31078733, 0.23955193, 0.41688854],\n       [0.43387498, 0.30767521, 0.10917941],\n       [0.73707129, 0.53718525, 0.55582339],\n       [0.65853255, 0.09792194, 0.64167559],\n       [0.75993109, 0.55770999, 0.87910348],\n       [0.14638011, 0.54246485, 0.38025993],\n       [0.92943834, 0.92955607, 0.89020598],\n       [0.62840535, 0.30418223, 0.61828077],\n       [0.84288483, 0.49795201, 0.29240602],\n       [0.02369341, 0.27068117, 0.49311519],\n       [0.78005137, 0.55611342, 0.95504498],\n       [0.03943504, 0.49609762, 0.76699066],\n       [0.76699066, 0.86465746, 0.16882263],\n       [0.31677058, 0.73284984, 0.79726112],\n       [0.21306895, 0.3158837 , 0.62957329],\n       [0.32762067, 0.13947277, 0.16485772],\n       [0.66704417, 0.24557385, 0.57877135],\n       [0.01782445, 0.85388333, 0.67431122],\n       [0.51677481, 0.57804745, 0.11780006],\n       [0.67945824, 0.43189576, 0.7924875 ],\n       [0.92173956, 0.98530942, 0.52901524],\n       [0.80507322, 0.15131171, 0.7669816 ],\n       [0.1028811 , 0.5614177 , 0.27183977],\n       [0.43080936, 0.9872328 , 0.12219063],\n       [0.30418222, 0.88696581, 0.50516188],\n       [0.48196445, 0.50604862, 0.76650333],\n       [0.59294862, 0.32081801, 0.67655247],\n       [0.96424485, 0.67856485, 0.47201186],\n       [0.5701827 , 0.94179547, 0.76735479],\n       [0.48984304, 0.6068396 , 0.7758134 ]])\n\n\n\nfrom torch import nn\n\nclass LinearModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearModel, self).__init__()\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        self.linear = nn.Linear(self.input_dim, self.output_dim, bias = True)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n    \n    def reset(self):\n        self.linear.reset_parameters()\n\n\nmodel = LinearModel(input_dim, output_dim)\nprint(model)\n\nLinearModel(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[0.3284]], requires_grad=True),\n Parameter containing:\n tensor([0.2677], requires_grad=True)]\n\n\n\nmodel.linear.weight\n\nParameter containing:\ntensor([[0.3284]], requires_grad=True)\n\n\n\nmodel.linear.bias\n\nParameter containing:\ntensor([0.2677], requires_grad=True)\n\n\n\nx = torch.randn(5, input_dim)\nmodel.forward(x)\n\ntensor([[ 0.3007],\n        [ 0.1968],\n        [-0.0247],\n        [-0.2091],\n        [ 0.0771]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n[model.linear.weight @ xx + model.linear.bias for xx in x]\n\n[tensor([0.3007], grad_fn=&lt;AddBackward0&gt;),\n tensor([0.1968], grad_fn=&lt;AddBackward0&gt;),\n tensor([-0.0247], grad_fn=&lt;AddBackward0&gt;),\n tensor([-0.2091], grad_fn=&lt;AddBackward0&gt;),\n tensor([0.0771], grad_fn=&lt;AddBackward0&gt;)]\n\n\n\nif input_dim == output_dim == 1:\n    fig = plt.figure()\n    fig.clf()\n    ax = fig.gca()\n    ax.plot(training_set.input_data, training_set.output_data, '.')\n    ax.plot(training_set.input_data, model.forward(training_set.input_data).detach().numpy(), '.')\n    ax.grid(True)\n    ax.set_xlabel('X_train')\n    ax.legend(['y_train', 'model (X_train)'])\n\n\n\n\n\nloss_fn = nn.MSELoss(reduction = 'mean')\n\n\nx = torch.tensor(np.array([1, 2, 1]).astype('f'))\nz = torch.tensor(np.array([0, 0, 0]).astype('f'))\n\nloss_fn(x, z)\n\ntensor(2.)\n\n\n\nx = torch.randn(1, input_dim)\ny = torch.randn(1, output_dim)\n\nmodel.zero_grad()\nloss = loss_fn(model.forward(x), y)\nloss.backward()\n\n\nif input_dim == output_dim == 1:\n    print(model.linear.weight.grad)\n    print(2 * x * (model.linear.weight * x + model.linear.bias - y))\n    \n    print(model.linear.bias.grad)\n    print(2 * (model.linear.weight * x + model.linear.bias - y))\n\ntensor([[1.2780]])\ntensor([[1.2780]], grad_fn=&lt;MulBackward0&gt;)\ntensor([3.1227])\ntensor([[3.1227]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "tutorial-3.html",
    "href": "tutorial-3.html",
    "title": "5  Graph Attention Networks",
    "section": "",
    "text": "6 Graph Attention Layer\nMessage passing network framework as a single formula:\n\\[x_i^{(k)} = \\gamma^{(k)}\\bigg(\\textbf{x}_i^{(k-1)}, \\sum\\limits_{j\\in N(i)}\\phi^{(k)}\\bigg(\\textbf{x}_i^{(k-1)}, \\textbf{x}_j^{(k-1)}, e_{j, 1}\\bigg)\\bigg)\\] where, - \\(x_i^{(k)}\\) is the feature representation of node \\(i\\) at the k-th layer of the computation graph. - \\(\\gamma^{(k)}\\) is a differentiable and trainable function - \\(\\sum\\) is a permutation invariant aggregation scheme - \\(\\bigg( , \\bigg)\\) is concatenation - \\(\\phi^{(k)}\\) is a differentiable and trainable function}\nPytorch Geometric comes with an in-built message passing base class. To implement a GAT-based network we must only supply it with a message function, an update function, and the aggregation scheme."
  },
  {
    "objectID": "tutorial-3.html#inputs-and-outputs",
    "href": "tutorial-3.html#inputs-and-outputs",
    "title": "5  Graph Attention Networks",
    "section": "6.1 Inputs and outputs",
    "text": "6.1 Inputs and outputs\nINPUT: a set of node features \\(h=\\{\\bar h_1, \\bar h_2, \\cdots, \\bar h_n\\}, \\bar h_i\\in \\mathbb{R}^F\\)\nOUTPUT: a set of node features \\(h'=\\{\\bar h'_1, \\bar h'_2, \\cdots, \\bar h'_n\\}, \\bar h'_i\\in \\mathbb{R}^{F'}\\)"
  },
  {
    "objectID": "tutorial-3.html#internal-computations-of-the-graph-attention-layer",
    "href": "tutorial-3.html#internal-computations-of-the-graph-attention-layer",
    "title": "5  Graph Attention Networks",
    "section": "6.2 Internal computations of the graph attention layer",
    "text": "6.2 Internal computations of the graph attention layer\n\nApply a parameterized linear transformation to every node. \\[\\textbf{w}.\\bar h_i, \\textbf{w}\\in\\mathbb{R}^{F'\\times F}\\]\nApply a self attention mechanism, \\(a\\), described as \\[\n\\begin{gather}\na:\\mathbb{R}^{F'}\\times \\mathbb{R}^{F'}\\to\\mathbb{R} \\\\\ne_{i, j} = a(\\textbf{w}\\cdot\\bar h_i, \\textbf{w}\\cdot\\bar h_j)\n\\end{gather}\\] The coeffecient \\(e_{i, j}\\) specify the importance of node \\(j\\)’s feature to node \\(i\\). where \\(a\\) is a single-layer feed forward neural network\nNormalize with respect all neighbors \\[\\alpha_{i, j} = \\text{softmax}_j(e_{i, j}) = \\frac{exp{e_{i, j}}}{\\sum\\limits_{k\\in N(i)} exp(e_{i, k})}\\]\nUse \\(\\alpha\\) in message passing \\[h'_i = \\sigma\\bigg(\\sum\\limits_{j\\in N(i)} \\alpha_{i, j} \\textbf{w}\\cdot h_j\\bigg)\\] In practice however we use multi-headed attention which involves calculating multiple overlapping values of \\(\\alpha\\) and using that in the formula instead. In Veliˇckovi´’s paper it is suggested to use concatenation in the inner layers of the network and an averaging mechanism for the final layer. These are expresed respectively in the following formulas: \\[\n\\begin{gather}\nh'_i = \\bigg|\\bigg|_{k=1}^K \\sigma\\bigg(\\sum\\limits_{j\\in N(i)}\\alpha_{i, j}^k\\textbf{w}^k h_j\\bigg) \\\\\nh'_i = \\sigma \\bigg( \\frac{1}{K}\\sum\\limits_{k=1}^K\\sum\\limits_{j\\in N(i)} \\alpha_{i, j}^k\\textbf{w}^k h_j\\bigg)\n\\end{gather}\n\\]"
  }
]