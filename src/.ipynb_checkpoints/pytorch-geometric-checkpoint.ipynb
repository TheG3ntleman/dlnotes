{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b308da",
   "metadata": {},
   "source": [
    "# What is pytorch geometric?\n",
    "Pytorch geometric is a library built on top of pytorch that aims to simplify the implementation of geometric deep learning models. First some motivation as to what geometric deep learning is.\n",
    "\n",
    "## Classical deep learning on graph-based data\n",
    "The naive approach to this problem is to find the adjacency matrix of a input graph and use that as input to a neural network as there are already several successful architectures that take matrices as input, primarily those related to computer vision (ex. convolutional neural networks, feed forward neural networks, ...). However, this approach has some problems:\n",
    "- Model cannot adapt to changes in graph size (I.E number of nodes). \n",
    "- The adjacency matrix is not permutation invariant (I.E one graph might have multiple distinct adjacency matrices).\n",
    "In many cases, not only is the structure of the graph important but also the properties of the nodes in a particular graph. Thus, in a general case, a network must also account for a secondary features matrix containing node related information.\n",
    "\n",
    "## Graph Neural Networks\n",
    "\n",
    "### The computation graph of a node\n",
    "Every node of a graph has a computation graph. The computation graph of a node is a tree where the root node is the node in question and it's children are it's neighbours. The neighbours in the child layer also have children (composed of the neighbours's neighbours). This process continuous until all nodes are exhuasted. **Node repetition is valid.**\n",
    "\n",
    "Sometimes, the process of generating the this computation graph layer by layer is informally referred to as unrolling the graph about a particular node. Often unrolling is limited to a particular number, this imposes a constraint on the depth of each computation graph.\n",
    "\n",
    "### Action on the computation graph of a node\n",
    "GNNs process graph-data by acting on these computation graphs via two mechanisms. One mechanism **aggregates** information whereas another **propagates** information. Processing begin from the leaf-nodes of a computation graph, where the data is directly supplied from the node-feature matrix. If multiple children have the same parent, their information is combined via this aggregation mechanism. This amalgmated product is then transmitted to the parent after further processes via the propagation mechanism which also considers the feature vector of the parent. This procedure continues till information reaches the root node.\n",
    "\n",
    "An importatnt property of the aggregation mechanism is that it is *permutation invariant* (I.E does not care about the order of inputs supplied).\n",
    "\n",
    "### How much to unroll\n",
    "Unrolling too little can result in important information not being transmitted whereas unrolling too much makes computation far more expensive even though the information transmitted from distant nodes might not affect the overall result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00057c05",
   "metadata": {},
   "source": [
    "# References\n",
    "1. [ Pytorch Geometric tutorial: Introduction to Pytorch geometric ](https://youtu.be/JtDgmmQ60x8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
