{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dda1075",
   "metadata": {},
   "source": [
    "# Graph  Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b98e76",
   "metadata": {},
   "source": [
    "The Graph Attention Network, or GAT, is a graph neural network which addresses the following questions:\n",
    "1. Of what importance are the features of a neighbouring node to the primary node?\n",
    "2. Can we learn this importance, in an automatic manner and regulate unrolling according to it?\n",
    "First we discuss the inputs and outputs of the graph attention Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c312c25",
   "metadata": {},
   "source": [
    "# Graph Attention Layer\n",
    "\n",
    "## Inputs and outputs\n",
    "**INPUT:** a set of node features $h=\\{\\bar h_1, \\bar h_2, \\cdots, \\bar h_n\\}, \\bar h_i\\in \\mathbb{R}^F$\n",
    "\n",
    "**OUTPUT:** a set of node features $h'=\\{\\bar h'_1, \\bar h'_2, \\cdots, \\bar h'_n\\}, \\bar h'_i\\in \\mathbb{R}^{F'}$\n",
    "\n",
    "## Internal computations of the graph attention layer\n",
    "\n",
    "1. Apply a parameterized linear transformation to every node. $$\\textbf{w}.\\bar h_i, \\textbf{w}\\in\\mathbb{R}^{F'\\times F}$$\n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb02c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
